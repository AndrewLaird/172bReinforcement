{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"FetchReach-v1\")\n",
    "DEFAULT_ENV_NAME=\"FetchReach-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "[0.09762701 0.43037874 0.20552675 0.08976637]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict(achieved_goal:Box(3,), desired_goal:Box(3,), observation:Box(10,))\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        # 4 input image channel, 32 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(16,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  \n",
    "        x= x.float()\n",
    "        return self.pipe(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10000\n",
    "REPLAY_START_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon decay, for choosing random actions to explore space\n",
    "EPSILON_DECAY_LAST_FRAME = 10**5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02\n",
    "#Going from 1 to .02 in 100000 frames\n",
    "MEAN_REWARD_BOUND=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define what experience is\n",
    "Experience = collections.namedtuple(\"Experience\", \n",
    "                        field_names = ['state','action','reward', 'done','new_state'])\n",
    "\n",
    "class ExperienceBuffer:\n",
    "     #This acts as our replay buffer\n",
    "    def __init__(self,capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity) #rotating buffer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        indices = np.random.choice(\n",
    "            len(self.buffer), batch_size, replace =False)\n",
    "        \n",
    "        states,actions,rewards,dones, next_states = \\\n",
    "            zip(*[self.buffer[idx] for idx in indices])\n",
    "        \n",
    "        states,actions = np.array(states),np.array(actions)\n",
    "        rewards = np.array(rewards,dtype=np.float32)\n",
    "        dones =np.array(dones, dtype=np.float32)\n",
    "        return states,actions,rewards,dones,next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make our agent that acts and learns in the environment\n",
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self):\n",
    "        state = env.reset()\n",
    "        \n",
    "        \n",
    "        self.state = np.concatenate(list(state.values()))\n",
    "        self.total_reward = 0.0\n",
    "    \n",
    "    def play_step(self, net, epsilong=0.0, device ='cpu'):\n",
    "        done_reward = None\n",
    "        \n",
    "        state_a = np.array([self.state], copy=False)\n",
    "        state_v = torch.tensor(state_a).to(device)\n",
    "            \n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "            #print(\"random action:\",action)\n",
    "            \n",
    "        else:\n",
    "            q_vals_v =  net(state_v)\n",
    "            #_, act_v = torch.max(q_vals_v,dim=1)\n",
    "            \n",
    "            \n",
    "            #action = int(act_v.item())\n",
    "            action = q_vals_v[0].detach().numpy()\n",
    "            #print(\"out action:\",action)\n",
    "            \n",
    "            \n",
    "        env.render()\n",
    "            \n",
    "        #take the action and update\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        new_state = np.concatenate(list(new_state.values()))\n",
    "\n",
    "        #calcululate how much closer to the center you are\n",
    "        new_state_torch =torch.tensor(new_state)\n",
    "        #print(\"state_v:\",state_v)\n",
    "        #print(\"new_state:\",new_state_torch.shape)\n",
    "        old_distance = torch.dist(state_v[10:13],state_v[13:16])\n",
    "        new_distance = torch.dist(new_state_torch[10:13],new_state_torch[13:16])\n",
    "\n",
    "        #if new distance is smaller we get a positive reward\n",
    "        reward = old_distance-new_distance \n",
    "\n",
    "        self.total_reward += reward\n",
    "        #do we need this? is it doing a copy?\n",
    "        #new_state = new_state\n",
    "\n",
    "        exp = Experience(self.state,action,reward,is_done,new_state)\n",
    "\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "\n",
    "        if(is_done):\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n",
    "        \n",
    "def calc_loss(batch, net, target_net, device='cpu'):\n",
    "    states,actions,rewards,dones,next_states = batch\n",
    "    \n",
    "    \n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "    #predictions\n",
    "    #look into gather further\n",
    "    state_action_values = net(states_v).gather(1,actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    #what is the value of being in the state that our action took us to?\n",
    "    next_state_values = target_net(next_states_v).max(1)[0] #take the max along the first axis\n",
    "\n",
    "\n",
    "    # saying the reward of the step after finishing is zero\n",
    "    # required to converege\n",
    "    next_state_values[done_mask] = 0.0\n",
    "\n",
    "    #we do not want to perform backprop on this \n",
    "    next_state_values = next_state_values.detach() \n",
    "\n",
    "    #-------------Bellman equation------------------\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (pipe): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #3 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5fd09042e843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2f8792f7d3c5>\u001b[0m in \u001b[0;36mcalc_loss\u001b[0;34m(batch, net, target_net, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m#predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m#look into gather further\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mstate_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m#what is the value of being in the state that our action took us to?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #3 'index'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument(\"--cude\", default=False, action='store_true', help=\"Enable cuda\")\n",
    "    \n",
    "    #parser.add_argument(\"--env\", default= DEFAULT_ENV_NAME, \n",
    "    #                    help=\"name of the environment, default=\"+DEFAULT_ENV_NAME)\n",
    "    \n",
    "    #parser.add_argument(\"--reward\", type=float, default=MEAN_REWARD_BOUND, \n",
    "    #                    help=\"Mean reward boundary for stop of training, default= %.2f\"% MEAN_REWARD_BOUND)\n",
    "    \n",
    "    #args = parser.parse_args()\n",
    "    args= [False,DEFAULT_ENV_NAME,MEAN_REWARD_BOUND]\n",
    "    #device= torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    device = 'cpu'\n",
    "    \n",
    "    env = gym.make(DEFAULT_ENV_NAME)#args.env)\n",
    "    net = DQN().to(device)\n",
    "    \n",
    "    target_net = DQN().to(device)\n",
    "    \n",
    "    writer = SummaryWriter(comment='-'+DEFAULT_ENV_NAME)#args.env)\n",
    "    print(net)\n",
    "    \n",
    "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "    agent = Agent(env, buffer)\n",
    "    \n",
    "    epsilon = EPSILON_START\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    total_rewards = []\n",
    "    frame_idx = 0\n",
    "    ts_frame = 0\n",
    "    ts = time.time()\n",
    "    \n",
    "    best_mean_reward = None\n",
    "    \n",
    "    while True:\n",
    "        frame_idx += 1\n",
    "        epsilon = max(EPSILON_FINAL, EPSILON_START - (frame_idx / EPSILON_DECAY_LAST_FRAME))\n",
    "        \n",
    "            \n",
    "        \n",
    "        reward = agent.play_step(net, epsilon, device=device)\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "            ts_frame = frame_idx\n",
    "            ts = time.time()\n",
    "            \n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "            #print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" %(\n",
    "            #        frame_idx, len(total_rewards), mean_reward, epsilon, speed\n",
    "            #    ))\n",
    "                \n",
    "            \n",
    "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "            writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "            \n",
    "            if best_mean_reward  is None or best_mean_reward < mean_reward:\n",
    "                torch.save(net.state_dict(), DEFAULT_ENV_NAME+\"-best.dat\")\n",
    "                if(best_mean_reward is not None):\n",
    "                    print('best mean reward updated %.3f -> %.3f, model saved'%\n",
    "                             (best_mean_reward, mean_reward))\n",
    "                    best_mean_reward = mean_reward\n",
    "                if(mean_reward > MEAN_REWARD_BOUND):\n",
    "                    print(\"solved in %d frames!\" % frame_idx)\n",
    "                    break\n",
    "                \n",
    "            if(len(buffer) < REPLAY_START_SIZE):\n",
    "                continue\n",
    "            \n",
    "            if(frame_idx % SYNC_TARGET_FRAMES == 0):\n",
    "                target_net.load_state_dict(net.state_dict())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            batch = buffer.sample(BATCH_SIZE)\n",
    "            loss_t = calc_loss(batch, net, target_net, device=device)\n",
    "            \n",
    "            loss_t.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
