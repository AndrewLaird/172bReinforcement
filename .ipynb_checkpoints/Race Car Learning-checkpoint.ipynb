{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"CarRacing-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    #rotating buffer of size N\n",
    "    def __init__(self,N,batch_size=5000):\n",
    "        self.memory = []\n",
    "        self.size = N\n",
    "        self.batch_size =batch_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        self.memory.append(experience)\n",
    "        if(len(self.memory) > self.size):\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def replay(self):    \n",
    "        #if(len(self.memory) < self.batch_size):\n",
    "        #    return []\n",
    "        #relay everything stored in small minibatches \n",
    "        # (currently of lenght 1)\n",
    "        # in a random order to reduce correlation\n",
    "        output = np.array(self.memory)\n",
    "        np.random.shuffle(output)\n",
    "        return output[:self.batch_size]\n",
    "\n",
    "#Define what our experience looks like\n",
    "#[state,action,reward,next_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN network from this tutorial \n",
    "# https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288\n",
    "# We will probably tweak this but our main interesting part is training this model\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(7056, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(type(x) != torch.Tensor):\n",
    "            x = torch.Tensor([[x]])\n",
    "            x = x.float()\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale_frame(frame):\n",
    "    return np.mean(frame,axis=2)\n",
    "\n",
    "def get_best_action(model,state):\n",
    "    #get our best action from our learner\n",
    "    #print(\"state given:\",state)\n",
    "    action = model.forward(state).detach().numpy()[0]\n",
    "    #action = int(action)\n",
    "    #print(\"State:\",state,\"Action:\",action)\n",
    "    action =  np.array(action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_from_data(model,target,data,optim):\n",
    "    for experience in data:\n",
    "        #compute the loss from\n",
    "        state,action,reward,next_state = experience\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #belman equation\n",
    "        \n",
    "        chosen_action_based_on_reward = model.forward(state)\n",
    "        expected_reward = reward + target.forward(next_state).detach_()\n",
    "        \n",
    "        loss = F.l1_loss(chosen_action_based_on_reward,expected_reward)\n",
    "        #we want the chosen_action based on reward to match \n",
    "        #the reward of being in the next state and the reward given\n",
    "        \n",
    "        #print(reward)\n",
    "        \n",
    "        # must zero gradients before backprop\n",
    "        # for pytorch\n",
    "        optim.zero_grad()\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(exploration_rate,state,model):\n",
    "    uniform_random_variable= random.uniform(0,1)\n",
    "        \n",
    "    if(uniform_random_variable > exploration_rate):\n",
    "        \n",
    "        #state = torch.tensor([[state]])\n",
    "        #state=state.float()\n",
    "\n",
    "        action = get_best_action(model,state)\n",
    "        \n",
    "    else:\n",
    "        #other wise explore randomly\n",
    "        action = env.action_space.sample()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(model,target):\n",
    "    target.load_state_dict(model.state_dict())\n",
    "    #print(\"model:\",model.state_dict())\n",
    "    #print(\"target:\",target.state_dict())\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training loop\n",
    "# Currently we do not have a target network but we could do that\n",
    "num_episodes = 1000000\n",
    "training_episodes = 2\n",
    "target_episodes = 15\n",
    "# network_update_epochs = 100 #use this if using a target network\n",
    "game_length = 10000\n",
    "\n",
    "# exploration rate, we want our network to explore \n",
    "# sometimes but not all the time, to do this\n",
    "# we use a decaying exploration rate\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.01\n",
    "# this bit of code will determine the decay\n",
    "# exploration_rate = min_exploration_rate + \\\n",
    "#        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=7056, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replay_memory = ReplayMemory(100000)\n",
    "dqn = DQN()#96,96)\n",
    "target = DQN()\n",
    "optimizer = optim.SGD(dqn.parameters(),lr=.01)\n",
    "print(dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = update_target(dqn,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploration_rate: 0.9901493354116764\n",
      "Episode: 2\n",
      "Track generation: 1183..1483 -> 300-tiles track\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fa1d7cf73446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sadly we have to render this env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexploration_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprev_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-706269900024>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(exploration_rate, state, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#state=state.float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_best_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7111175cb612>\u001b[0m in \u001b[0;36mget_best_action\u001b[0;34m(model, state)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#print(\"State:\",state,\"Action:\",action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    print(\"Episode:\",episode)\n",
    "    state= env.reset()\n",
    "    state = grayscale_frame(state)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    tko_timer = 0\n",
    "    game = []\n",
    "    \n",
    "    for step in range(game_length):\n",
    "        #env.render() #sadly we have to render this env\n",
    "        \n",
    "        action = select_action(exploration_rate,state,dqn)\n",
    "        \n",
    "        prev_state = state\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if(reward <= -.1):\n",
    "            tko_timer +=1\n",
    "            \n",
    "        else:\n",
    "            tko_timer = 0\n",
    "            \n",
    "        #remove the negative constant penalty\n",
    "        reward = int(10*reward + 1)\n",
    "        state = grayscale_frame(state)\n",
    "        \n",
    "        experience= [prev_state,action,reward,state]\n",
    "        game.append(experience)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #replay_memory.add(experience)\n",
    "        \n",
    "        #tracking how far we are getting\n",
    "        episode_reward +=reward\n",
    "        \n",
    "        if(done or tko_timer >=100):\n",
    "            # if we haven't gotten a positive reward \n",
    "            # in the last 20 steps \n",
    "            # or the game is over: stop\n",
    "            if(step > 25):\n",
    "                [replay_memory.add(e) for e in game]\n",
    "            break\n",
    "        \n",
    "    \n",
    "    #end of episode variable unm pdating\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    print(\"exploration_rate:\",exploration_rate)\n",
    "    \n",
    "    if(episode%training_episodes == 0 and episode != 0):\n",
    "        print(\"Training on Experience\")\n",
    "        dqn = learn_from_data(dqn,target,replay_memory.replay(),optimizer)\n",
    "    if(episode%target_episodes ==0):\n",
    "        print(\"updating the target network\")\n",
    "        target = update_target(dqn,target)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
