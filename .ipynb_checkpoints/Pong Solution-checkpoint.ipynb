{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " class ReplayMemory:\n",
    "    #rotating buffer of size N\n",
    "    def __init__(self,N,batch_size=500):\n",
    "        self.memory = []\n",
    "        self.size = N\n",
    "        self.batch_size =batch_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        self.memory.append(experience)\n",
    "        if(len(self.memory) > self.size):\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def replay(self):    \n",
    "        \n",
    "        # we want to keep some information about direction though \n",
    "        # so we will give 3 frames at a time\n",
    "        \n",
    "        buffer_size = len(self.memory)\n",
    "        output = np.array(self.memory)\n",
    "        \n",
    "        if(buffer_size < self.batch_size):\n",
    "            return []\n",
    "        \n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = self.batch_size,\n",
    "                                replace = False)\n",
    "        return output[index]\n",
    "\n",
    "#Define what our experience looks like\n",
    "#[state,action,reward,next_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN network from this tutorial \n",
    "# https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288\n",
    "# We will probably tweak this but our main interesting part is training this model\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        # 4 input image channel, 32 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(5, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(21904, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut off the infromation on the top and grayscale\n",
    "def format_frame(frame):\n",
    "    frame =np.mean(frame,axis=2)\n",
    "    frame = frame[35:-15]\n",
    "    return frame\n",
    "\n",
    "def get_most_likely_action(action_confidences):\n",
    "    #returns the index of the most likely action\n",
    "    return np.argmax(action_confidences)\n",
    "\n",
    "def to_onehot(index,size):\n",
    "    #makes a onehot array of size size\n",
    "    # with the index index 1 and all others 0\n",
    "    onehot = torch.zeros(size)\n",
    "    onehot[index] = 1\n",
    "    return onehot\n",
    "\n",
    "def format_frames(stacked_frames):\n",
    "    # turn a list of numpy arrays into \n",
    "    # a torch tensor\n",
    "    torch_frames = [torch.from_numpy(n).float() for n in stacked_frames]\n",
    "    #now stack them into one big tensor\n",
    "    output_tensor = torch.stack(torch_frames)\n",
    "    output_tensor = torch.reshape(output_tensor,(1,len(torch_frames),160,160))\n",
    "    return output_tensor\n",
    "\n",
    "def get_best_action(model,stacked_frames):\n",
    "    #get our best action from our learner\n",
    "    #print(\"state given:\",state)\n",
    "    frames_tensor = format_frames(stacked_frames)\n",
    "    \n",
    "    action = model.forward(frames_tensor).detach().numpy()[0]\n",
    "    #action = int(action)\n",
    "    #print(\"State:\",state,\"Action:\",action)\n",
    "    action = np.array(action)\n",
    "    action = get_most_likely_action(action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_actions = 6\n",
    "\n",
    "def learn_from_data(model,target,data,optim):\n",
    "    for experience in data:\n",
    "        #compute the loss from\n",
    "        state,action,reward,next_state = experience\n",
    "        #make the action match what the network outputs\n",
    "        \n",
    "        #belman equation\n",
    "        state = format_frames(state)\n",
    "        next_state = format_frames(next_state)\n",
    "        \n",
    "        chosen_action_based_on_reward = model.forward(state)\n",
    "        \n",
    "        expected_reward = target.forward(next_state).detach_()\n",
    "        \n",
    "        reward_tensor = torch.zeros(number_of_actions)\n",
    "        reward_tensor[action] = reward\n",
    "        \n",
    "        #reward_tensor.float()\n",
    "        #print(reward_tensor)\n",
    "        \n",
    "        expected_reward += reward_tensor\n",
    "        \n",
    "        loss = F.l1_loss(chosen_action_based_on_reward,expected_reward)\n",
    "        #we want the chosen_action based on reward to match \n",
    "        #the reward of being in the next state and the reward given\n",
    "        \n",
    "        #print(reward)\n",
    "        \n",
    "        # must zero gradients before backprop\n",
    "        # for pytorch\n",
    "        optim.zero_grad()\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(exploration_rate,state,model):\n",
    "    uniform_random_variable= random.uniform(0,1)\n",
    "        \n",
    "    if(uniform_random_variable > exploration_rate):\n",
    "        \n",
    "        #state = torch.tensor([[state]])\n",
    "        #state=state.float()\n",
    "\n",
    "        action = get_best_action(model,state)\n",
    "        \n",
    "    else:\n",
    "        #other wise explore randomly\n",
    "        action = env.action_space.sample()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(model,target):\n",
    "    target.load_state_dict(model.state_dict())\n",
    "    #print(\"model:\",model.state_dict())\n",
    "    #print(\"target:\",target.state_dict())\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training loop\n",
    "# Currently we do not have a target network but we could do that\n",
    "num_episodes = 1000000\n",
    "training_episodes = 10\n",
    "target_episodes = 15\n",
    "# network_update_epochs = 100 #use this if using a target network\n",
    "game_length = 10000\n",
    "\n",
    "# exploration rate, we want our network to explore \n",
    "# sometimes but not all the time, to do this\n",
    "# we use a decaying exploration rate\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.01\n",
    "# this bit of code will determine the decay\n",
    "# exploration_rate = min_exploration_rate + \\\n",
    "#        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv1): Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=21904, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replay_memory = ReplayMemory(1000)\n",
    "dqn = DQN()#96,96)\n",
    "target = DQN()\n",
    "optimizer = optim.SGD(dqn.parameters(),lr=.01)\n",
    "print(dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = update_target(dqn,target)\n",
    "\n",
    "def play_game(dqn,replay_memory,env,bounding_reward = True,show=False):\n",
    "    time.sleep(.1)\n",
    "    frame= env.reset()\n",
    "    frame = format_frame(frame)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    tko_timer = 0\n",
    "    game = []\n",
    "    \n",
    "    action_frequency = 5\n",
    "    \n",
    "    stacked_frames = []\n",
    "    previous_frames = []\n",
    "    stacked_reward = 0\n",
    "    #stacked_frames.append(frame)\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    \n",
    "    for step in range(game_length):\n",
    "        if(show==True):\n",
    "            env.render()\n",
    "        #only make a new desision every 5 frames\n",
    "        \n",
    "        frame, reward, done, info = env.step(action)\n",
    "        \n",
    "        \n",
    "        frame = format_frame(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        #env.render() we could render to show ourseleves\n",
    "        if(step % action_frequency==0 and step != 0):\n",
    "            action = select_action(exploration_rate,stacked_frames,dqn)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if(reward < 0 and bounding_reward):\n",
    "            reward = 0\n",
    "        \n",
    "        stacked_reward += reward\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(step % action_frequency==0 and step > action_frequency):\n",
    "            experience= [previous_frames,action,stacked_reward,stacked_frames]\n",
    "            replay_memory.add(experience)\n",
    "        \n",
    "        if(step % action_frequency==0):\n",
    "            previous_frames=stacked_frames\n",
    "            stacked_frames = []\n",
    "            stacked_reward = 0\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "         \n",
    "        \n",
    "        \n",
    "        #tracking how far we are getting\n",
    "        episode_reward +=reward\n",
    "        \n",
    "        if(done):\n",
    "            # if we haven't gotten a positive reward \n",
    "            # in the last 20 steps \n",
    "            # or the game is over: stop\n",
    "            break\n",
    "    return episode_reward\n",
    "    \n",
    "def show_progress(rewards):\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    print(\"exploration_rate:\",exploration_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploration_rate: 0.970741078213023\n",
      "Episode: 4\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    print(\"Episode:\",episode)\n",
    "    \n",
    "    #the play_game will update replay_memory to include the game\n",
    "    episode_reward = play_game(dqn,replay_memory,env,bounding_reward = False,show=False)\n",
    "    \n",
    "    \n",
    "    #end of episode variable updating\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "    \n",
    "    show_progress(rewards)\n",
    "    \n",
    "    if(episode % training_episodes == 0 and episode != 0):\n",
    "        print(\"Training on Experience\")\n",
    "        dqn = learn_from_data(dqn,target,replay_memory.replay(),optimizer)\n",
    "    \n",
    "    if(episode%target_episodes ==0):\n",
    "        print(\"updating the target network\")\n",
    "        target = update_target(dqn,target)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
