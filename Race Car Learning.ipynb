{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"CarRacing-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    #rotating buffer of size N\n",
    "    def __init__(self,N,batch_size=5000):\n",
    "        self.memory = []\n",
    "        self.size = N\n",
    "        self.batch_size =batch_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        self.memory.append(experience)\n",
    "        if(len(self.memory) > self.size):\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def replay(self):    \n",
    "        #if(len(self.memory) < self.batch_size):\n",
    "        #    return []\n",
    "        #relay everything stored in small minibatches \n",
    "        # (currently of lenght 1)\n",
    "        # in a random order to reduce correlation\n",
    "        output = np.array(self.memory)\n",
    "        np.random.shuffle(output)\n",
    "        return output[:self.batch_size]\n",
    "\n",
    "#Define what our experience looks like\n",
    "#[state,action,reward,next_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN network from this tutorial \n",
    "# https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288\n",
    "# We will probably tweak this but our main interesting part is training this model\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(7056, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(type(x) != torch.Tensor):\n",
    "            x = torch.Tensor([[x]])\n",
    "            x = x.float()\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale_frame(frame):\n",
    "    return np.mean(frame,axis=2)\n",
    "\n",
    "def get_best_action(model,state):\n",
    "    #get our best action from our learner\n",
    "    #print(\"state given:\",state)\n",
    "    action = model.forward(state).detach().numpy()[0]\n",
    "    #action = int(action)\n",
    "    #print(\"State:\",state,\"Action:\",action)\n",
    "    action =  np.array(action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_from_data(model,target,data,optim):\n",
    "    for experience in data:\n",
    "        #compute the loss from\n",
    "        state,action,reward,next_state = experience\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #belman equation\n",
    "        \n",
    "        chosen_action_based_on_reward = model.forward(state)\n",
    "        expected_reward = reward + target.forward(next_state).detach_()\n",
    "        \n",
    "        loss = F.l1_loss(chosen_action_based_on_reward,expected_reward)\n",
    "        #we want the chosen_action based on reward to match \n",
    "        #the reward of being in the next state and the reward given\n",
    "        \n",
    "        #print(reward)\n",
    "        \n",
    "        # must zero gradients before backprop\n",
    "        # for pytorch\n",
    "        optim.zero_grad()\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(exploration_rate,state,model):\n",
    "    uniform_random_variable= random.uniform(0,1)\n",
    "        \n",
    "    if(uniform_random_variable > exploration_rate):\n",
    "        \n",
    "        #state = torch.tensor([[state]])\n",
    "        #state=state.float()\n",
    "\n",
    "        action = get_best_action(model,state)\n",
    "        \n",
    "    else:\n",
    "        #other wise explore randomly\n",
    "        action = env.action_space.sample()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(model,target):\n",
    "    target.load_state_dict(model.state_dict())\n",
    "    #print(\"model:\",model.state_dict())\n",
    "    #print(\"target:\",target.state_dict())\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training loop\n",
    "# Currently we do not have a target network but we could do that\n",
    "num_episodes = 1000000\n",
    "training_episodes = 2\n",
    "target_episodes = 15\n",
    "# network_update_epochs = 100 #use this if using a target network\n",
    "game_length = 10000\n",
    "\n",
    "# exploration rate, we want our network to explore \n",
    "# sometimes but not all the time, to do this\n",
    "# we use a decaying exploration rate\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.01\n",
    "# this bit of code will determine the decay\n",
    "# exploration_rate = min_exploration_rate + \\\n",
    "#        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=7056, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replay_memory = ReplayMemory(100000)\n",
    "dqn = DQN()#96,96)\n",
    "target = DQN()\n",
    "optimizer = optim.SGD(dqn.parameters(),lr=.01)\n",
    "print(dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = update_target(dqn,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploration_rate: 0.8706646530448178\n",
      "Training on Experience\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    print(\"Episode:\",episode)\n",
    "    state= env.reset()\n",
    "    state = grayscale_frame(state)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    tko_timer = 0\n",
    "    game = []\n",
    "    \n",
    "    for step in range(game_length):\n",
    "        #env.render() #sadly we have to render this env\n",
    "        \n",
    "        action = select_action(exploration_rate,state,dqn)\n",
    "        \n",
    "        prev_state = state\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if(reward <= -.1):\n",
    "            tko_timer +=1\n",
    "            \n",
    "        else:\n",
    "            tko_timer = 0\n",
    "            \n",
    "        #remove the negative constant penalty\n",
    "        reward = int(10*reward + 1)\n",
    "        state = grayscale_frame(state)\n",
    "        \n",
    "        experience= [prev_state,action,reward,state]\n",
    "        game.append(experience)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #replay_memory.add(experience)\n",
    "        \n",
    "        #tracking how far we are getting\n",
    "        episode_reward +=reward\n",
    "        \n",
    "        if(done or tko_timer >=100):\n",
    "            # if we haven't gotten a positive reward \n",
    "            # in the last 20 steps \n",
    "            # or the game is over: stop\n",
    "            if(step > 25):\n",
    "                [replay_memory.add(e) for e in game]\n",
    "            break\n",
    "        \n",
    "    \n",
    "    #end of episode variable unm pdating\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    print(\"exploration_rate:\",exploration_rate)\n",
    "    \n",
    "    if(episode%training_episodes == 0 and episode != 0):\n",
    "        print(\"Training on Experience\")\n",
    "        dqn = learn_from_data(dqn,target,replay_memory.replay(),optimizer)\n",
    "    if(episode%target_episodes ==0):\n",
    "        print(\"updating the target network\")\n",
    "        target = update_target(dqn,target)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
